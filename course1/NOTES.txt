we are not going to solve the knapsack problem yet by writing a program, but we will.

remember, SHIFT + ENTER adds a new line in the console without executing the code.

the ‘best possible solution’ dep ends on your circumstances, but generally speaking is the best performance, so it takes the least amount of time. therefore, how do we measure performance?

in JS there’s a ‘performance’ object in which we can call the ‘now’ method, which gives us the current time stamp

to copy again the same in the console, you can use the up arrow key.

you try the same function with different arguments, and see how time changes.

when you use hard numbers (different numbers as inputs), a lot of factors (that have nothing to do with the algorithm) influence the amount of time it takes (new or old computer, how many processors running in the background, or in the browser, etc. also the browser does some optimizations behind the scene (it caches the function, values, etc.)), so the output may vary, even using the same inputs. therefore, measuring time like this is not a great idea, specially for low values. but still, if you use bigger and bigger numbers, you will start to see a trend (see screenshot)

therefore, we shouldn’t care much about the specific numbers (since these depend a lot on the environment, especially the low numbers), but we should care about the general trend (especially for the large numbers)

in our example, time increases in a linear way: the factor by which we increase the input (n), is the factor by which the time the function takes increases (also n).

this is how we judge performance when we talk about algorithms: we don’t care about the specific numbers but about the function of time (the trend) (generally speaking, the time it takes in relation to the inputs). we also call this ‘time complexity’. in our example, our algorithm has linear time complexity.

remember, you often have more than one way of solving a problem, ie. different algorithms that you can use to solve the problem (if we wouldn’t have different ways to solve a problem, then we wouldn’t need to measure and compare the performance of different algorithms)

constant time: the number of inputs doesn’t have any influence on the time that the algorithm takes. for example, the mathematical formula to solve our problem (see screenshot): there is no loop. if you measure the time the algorithm takes with different inputs, you will see that it's basically the same time. therefore, the performance of this algorithm is much better!

you could say: this algorithm has linear time complexity, or constant time complexity, etc., but we also use something called the big O notation, which makes it easier for us to express the time complexity of a given algorithm to other developers. Big O notation looks like this: O(n), O(1), etc. You can use the big O notation to add annotations in your code, or answer your interviewer, etc. Big O notation is the standard way to compare performance between algorithms.

using performance.now(), we can derive the time complexity easily for linear time complexity, because it's quite intuitive, but this doesn't happen for other time complexities (keep in mind that the time you get from performance.now() is also influenced by other factors, therefore you could think you have certain time complexity (for example, linear), but in reality there's a distorsion, so you have another time complexity).

to derive time complexity in an accurate way, we use a technique called asymptotic analysis, which involve 3 steps: first, define the mathematical time comoplexity function (that correlates the inputs and the time), by counting the number of expression executions (you look at each expression in your algorithm, ie. roughly speaking, every line of code, and count how often it is getting executed); note that we are not measuring the specific time, since that's influenced by many factors. instead, we assume that each expression takes roughly the same amount of time. therefore, we can compare the number of expression executions of our algorithm with another algorithm (see screenshot).
(note that the initialization of the for loop is executed only once, in our example)
at the end of the screenshot, we express the time complexity with 'T', and we can use this expression to compare this algorithm to other algorithms.
but we don't need to be that exact: for example, if we added a console.log in our function, we would still count it, but it doesn't really matter, we care about the general function, the kind of function, to be precise. therefore, you can generalize the 'T' expression with coeficients (in our example, 'a' and 'b'), because specific numbers don't matter, what matters is the big picture. in step 3, we remove the coeficient because what we care is the general picture.

(when we find the fastest growing term for constant time complexity, 1 is not growing, but it's the only term we have, and there's no coeficient to remove)

we remove the coeficient because we just care about the general trend. that's why, if we added a console.log line in the constant time complexity function, we woulnd't say 2, we say just 1, because we remove the coeficient.(for the same reason, we focus on the fastest growing term, because we focus on the general trend): we don't care about the number of code executions, we just want to see the general trend.

constant time complexity is the best possible case for an algorithm. in our example we have this mathematical formula that gives us constant time complexity, so we have a better alternative to the algorithm with linear time complexity, but this is not always possible.

REMINDER: we derive the big O notations in order to compare different algorithms, because we can't rely on the hard numbers of milliseconds. that's why we have the big O notation that tells us which kind of algorithm that is, which kind of function it has.

in the screenshot, the time complexities are ordered from fast to slow.
logarithmic: maybe initially it's slower than linear time complexity, but with bigger n it tends to be faster, because it doesn't grow linearly, it grows at a slower pace. the slowest time complexity we have in the screenshot is the exponential one: it will quickly become impossible to execute, for very low n values we will start to see problems.
these are not the only time complexities we have. in theory, any mathematical function or construct is possible, but the most common are linear, constant and quadratic, and also logarithmic.

PROBLEM: the lowest possible time complexity is the fastest solution.
if the problem told us that we have a fixed number of elements in the array, then the best solution would be the one in the screenshot, because it has O(1)
(look at the console.log inside the loop in the screenshot, here you can see that when you eliminate coeficients, you will end up with the general trend)
after we find the solutions with O(n), can we do better? that's not so easy to answer, since you don't know all possible solutions to all problems. but is there anything we can improve in our current code? let's focus on the part of our code that depends on n, since we cannot improve the other lines that run only once. but if we could get rid of the loop, we would improve a lot. however, for this problem, if we don't know the length of the array in advance (if we are not told about that in the problem formulation), there's no way to get rid of the loop.
the solution with the 'reduce' method, even though it's one line, it will internally do something similar to the loop (it's not like the mathematical formula that we had in the former example of O(1)): JUST BECAUSE WE WERE ABLE TO REDUCE OUR CODE TO ONE LINE, IT DOESN'T MAKE IT TO O(1) NECESSARILY. THAT'S ONLY TRUE IF IN THAT LINE YOU ARE NOT CALLING A NEW FUNCTION. therefore, we would need to find what the time complexity of the 'reduce' method is, in order to find out about the time complexity of this algorithm.('reduce' will also give us linear time complexity). THEREFORE, THE LOOP SOLUTION WITH LINEAR TIME COMPLEXITY IS THE BEST AGORITHM WE CAN COME UP WITH FOR THIS PROBLEM.
